{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ed0a269-380d-4a39-afd2-40baa9c092e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =========================================\n",
    "# Notebook: 05_dq_audit_report\n",
    "# Purpose : Data Quality + Audit Report (judge-friendly)\n",
    "# =========================================\n",
    "%run ./01_config\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def write_dq(dataset, layer, rule_name, severity, df_violations, key_cols):\n",
    "    \"\"\"\n",
    "    Stores aggregated DQ violation info into audit.dq_violations with sample keys.\n",
    "    \"\"\"\n",
    "    cnt = df_violations.count()\n",
    "    if cnt == 0:\n",
    "        return\n",
    "\n",
    "    # sample up to 20 keys as a compact string\n",
    "    sample = (df_violations\n",
    "        .select(*[F.col(c).cast(\"string\") for c in key_cols if c in df_violations.columns])\n",
    "        .limit(20)\n",
    "        .toPandas()\n",
    "    )\n",
    "    sample_keys = sample.to_csv(index=False).strip().replace(\"\\n\", \" | \") if len(sample) > 0 else \"\"\n",
    "\n",
    "    out = spark.createDataFrame([(\n",
    "        BATCH_ID, layer, dataset, rule_name, severity, int(cnt), sample_keys\n",
    "    )], schema=\"batch_id string, layer string, dataset string, rule_name string, severity string, violation_count long, sample_keys string\")\n",
    "\n",
    "    (out\n",
    "     .withColumn(\"created_ts\", F.current_timestamp())\n",
    "     .write.format(\"delta\").mode(\"append\").saveAsTable(f\"{DB_AUDIT}.dq_violations\")\n",
    "    )\n",
    "\n",
    "def metric(table_fqn, layer, name, value):\n",
    "    audit_table_metrics(table_fqn, layer, name, str(value))\n",
    "\n",
    "# -----------------------------------------\n",
    "# Load tables\n",
    "# -----------------------------------------\n",
    "sm  = spark.table(f\"{DB_SILVER}.smart_meter_readings\")\n",
    "tel = spark.table(f\"{DB_SILVER}.substation_telemetry\")\n",
    "bill = spark.table(f\"{DB_SILVER}.billing_transactions_current\")\n",
    "cust = spark.table(f\"{DB_SILVER}.dim_customer_current\")\n",
    "trf  = spark.table(f\"{DB_SILVER}.dim_transformer\")  # full history\n",
    "trf_cur = spark.table(f\"{DB_SILVER}.dim_transformer_current\")\n",
    "gold_ops = spark.table(f\"{DB_GOLD}.ops_daily_dashboard\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1) Core DQ Rules — Smart Meter\n",
    "# -----------------------------------------\n",
    "write_dq(\n",
    "    dataset=\"smart_meter_readings\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"PK_NOT_NULL_event_id\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=sm.filter(F.col(\"event_id\").isNull() | (F.trim(\"event_id\") == \"\")),\n",
    "    key_cols=[\"meter_id\",\"event_ts\",\"region\"]\n",
    ")\n",
    "\n",
    "write_dq(\n",
    "    dataset=\"smart_meter_readings\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"PK_NOT_NULL_meter_id\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=sm.filter(F.col(\"meter_id\").isNull() | (F.trim(\"meter_id\") == \"\")),\n",
    "    key_cols=[\"event_id\",\"event_ts\",\"region\"]\n",
    ")\n",
    "\n",
    "write_dq(\n",
    "    dataset=\"smart_meter_readings\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"NEGATIVE_KWH\",\n",
    "    severity=\"WARNING\",\n",
    "    df_violations=sm.filter(F.col(\"kwh_consumed\") < 0),\n",
    "    key_cols=[\"event_id\",\"meter_id\",\"kwh_consumed\"]\n",
    ")\n",
    "\n",
    "write_dq(\n",
    "    dataset=\"smart_meter_readings\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"INVALID_TIMESTAMP\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=sm.filter(F.col(\"event_ts\").isNull()),\n",
    "    key_cols=[\"event_id\",\"meter_id\"]\n",
    ")\n",
    "\n",
    "# Voltage plausibility (broad bounds)\n",
    "write_dq(\n",
    "    dataset=\"smart_meter_readings\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"VOLTAGE_OUT_OF_RANGE\",\n",
    "    severity=\"WARNING\",\n",
    "    df_violations=sm.filter((F.col(\"voltage_v\") < 150) | (F.col(\"voltage_v\") > 300)),\n",
    "    key_cols=[\"event_id\",\"meter_id\",\"voltage_v\",\"region\"]\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Core DQ Rules — Telemetry\n",
    "# -----------------------------------------\n",
    "write_dq(\n",
    "    dataset=\"substation_telemetry\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"PK_NOT_NULL_telemetry_id\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=tel.filter(F.col(\"telemetry_id\").isNull() | (F.trim(\"telemetry_id\") == \"\")),\n",
    "    key_cols=[\"transformer_id\",\"telemetry_ts\",\"region\"]\n",
    ")\n",
    "\n",
    "write_dq(\n",
    "    dataset=\"substation_telemetry\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"PK_NOT_NULL_transformer_id\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=tel.filter(F.col(\"transformer_id\").isNull() | (F.trim(\"transformer_id\") == \"\")),\n",
    "    key_cols=[\"telemetry_id\",\"telemetry_ts\",\"region\"]\n",
    ")\n",
    "\n",
    "write_dq(\n",
    "    dataset=\"substation_telemetry\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"INVALID_TIMESTAMP\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=tel.filter(F.col(\"telemetry_ts\").isNull()),\n",
    "    key_cols=[\"telemetry_id\",\"transformer_id\",\"region\"]\n",
    ")\n",
    "\n",
    "# Oil temp delta plausibility (loose)\n",
    "write_dq(\n",
    "    dataset=\"substation_telemetry\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"OIL_TEMP_DELTA_IMPLAUSIBLE\",\n",
    "    severity=\"WARNING\",\n",
    "    df_violations=tel.filter(F.col(\"oil_temp_delta_c\") > 80),\n",
    "    key_cols=[\"telemetry_id\",\"transformer_id\",\"oil_temp_delta_c\",\"region\"]\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3) Core DQ Rules — Billing (current-state)\n",
    "# -----------------------------------------\n",
    "write_dq(\n",
    "    dataset=\"billing_transactions_current\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"PK_NOT_NULL_transaction_id\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=bill.filter(F.col(\"transaction_id\").isNull() | (F.trim(\"transaction_id\") == \"\")),\n",
    "    key_cols=[\"customer_id\",\"cdc_ts\"]\n",
    ")\n",
    "\n",
    "# Amount due plausibility (if present)\n",
    "amt_col = None\n",
    "for c in [\"amount_due\",\"amount\",\"total_amount\",\"bill_amount\"]:\n",
    "    if c in bill.columns:\n",
    "        amt_col = c\n",
    "        break\n",
    "\n",
    "if amt_col:\n",
    "    write_dq(\n",
    "        dataset=\"billing_transactions_current\",\n",
    "        layer=\"SILVER\",\n",
    "        rule_name=\"NEGATIVE_AMOUNT\",\n",
    "        severity=\"WARNING\",\n",
    "        df_violations=bill.filter(F.col(amt_col) < 0),\n",
    "        key_cols=[\"transaction_id\",\"customer_id\",amt_col]\n",
    "    )\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4) SCD2 sanity checks — Transformer dimension\n",
    "# -----------------------------------------\n",
    "# Rule: Only one current record per transformer_id\n",
    "multi_current = (trf\n",
    "    .filter(\"is_current = true\")\n",
    "    .groupBy(\"transformer_id\")\n",
    "    .count()\n",
    "    .filter(\"count > 1\")\n",
    ")\n",
    "\n",
    "write_dq(\n",
    "    dataset=\"dim_transformer\",\n",
    "    layer=\"SILVER\",\n",
    "    rule_name=\"SCD2_MULTIPLE_CURRENT_RECORDS\",\n",
    "    severity=\"CRITICAL\",\n",
    "    df_violations=multi_current,\n",
    "    key_cols=[\"transformer_id\",\"count\"]\n",
    ")\n",
    "\n",
    "# Rule: current record must have end_date = 9999-12-31 (if column exists)\n",
    "if \"record_end_date\" in trf.columns:\n",
    "    wrong_end = trf.filter(\"is_current = true AND record_end_date <> to_date('9999-12-31')\")\n",
    "    write_dq(\n",
    "        dataset=\"dim_transformer\",\n",
    "        layer=\"SILVER\",\n",
    "        rule_name=\"SCD2_CURRENT_END_DATE_NOT_MAX\",\n",
    "        severity=\"WARNING\",\n",
    "        df_violations=wrong_end,\n",
    "        key_cols=[\"transformer_id\",\"record_end_date\",\"scd_version\"]\n",
    "    )\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Join Coverage / Referential sanity checks (Gold readiness)\n",
    "# customer ↔ meter join coverage if meter_id exists in customer dim :contentReference[oaicite:0]{index=0}\n",
    "# -----------------------------------------\n",
    "if \"meter_id\" in cust.columns:\n",
    "    joined = (sm.select(\"meter_id\").distinct()\n",
    "              .join(cust.select(\"meter_id\").distinct(), on=\"meter_id\", how=\"left\")\n",
    "              .withColumn(\"has_customer\", F.col(\"meter_id\").isNotNull() & F.col(\"meter_id\").isNotNull())\n",
    "             )\n",
    "    # measure coverage as: meters in readings that exist in customer dim\n",
    "    coverage = (sm.select(\"meter_id\").distinct()\n",
    "                .join(cust.select(\"meter_id\").distinct().withColumn(\"hit\", F.lit(1)), \"meter_id\", \"left\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"meters_in_readings\"),\n",
    "                    F.sum(F.coalesce(\"hit\", F.lit(0))).alias(\"meters_with_customer\")\n",
    "                )\n",
    "                .collect()[0]\n",
    "               )\n",
    "    meters_in_readings = coverage[\"meters_in_readings\"]\n",
    "    meters_with_customer = coverage[\"meters_with_customer\"]\n",
    "    cov_rate = meters_with_customer / meters_in_readings if meters_in_readings else 0.0\n",
    "\n",
    "    metric(f\"{DB_SILVER}.smart_meter_readings\", \"SILVER\", \"meter_to_customer_join_coverage\", f\"{cov_rate:.4f}\")\n",
    "\n",
    "# telemetry ↔ transformer join coverage\n",
    "tel_cov = (tel.select(\"transformer_id\").distinct()\n",
    "    .join(trf_cur.select(\"transformer_id\").distinct().withColumn(\"hit\", F.lit(1)), \"transformer_id\", \"left\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"transformers_in_telemetry\"),\n",
    "        F.sum(F.coalesce(\"hit\", F.lit(0))).alias(\"transformers_found_in_dim\")\n",
    "    )\n",
    "    .collect()[0]\n",
    ")\n",
    "tc_total = tel_cov[\"transformers_in_telemetry\"]\n",
    "tc_hit = tel_cov[\"transformers_found_in_dim\"]\n",
    "tc_rate = tc_hit / tc_total if tc_total else 0.0\n",
    "metric(f\"{DB_SILVER}.substation_telemetry\", \"SILVER\", \"telemetry_to_transformer_join_coverage\", f\"{tc_rate:.4f}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 6) Pipeline health summary metrics (min/max timestamps)\n",
    "# -----------------------------------------\n",
    "sm_ts = sm.agg(F.min(\"event_ts\").alias(\"min_ts\"), F.max(\"event_ts\").alias(\"max_ts\")).collect()[0]\n",
    "tel_ts = tel.agg(F.min(\"telemetry_ts\").alias(\"min_ts\"), F.max(\"telemetry_ts\").alias(\"max_ts\")).collect()[0]\n",
    "\n",
    "metric(f\"{DB_SILVER}.smart_meter_readings\", \"SILVER\", \"min_event_ts\", sm_ts[\"min_ts\"])\n",
    "metric(f\"{DB_SILVER}.smart_meter_readings\", \"SILVER\", \"max_event_ts\", sm_ts[\"max_ts\"])\n",
    "metric(f\"{DB_SILVER}.substation_telemetry\", \"SILVER\", \"min_telemetry_ts\", tel_ts[\"min_ts\"])\n",
    "metric(f\"{DB_SILVER}.substation_telemetry\", \"SILVER\", \"max_telemetry_ts\", tel_ts[\"max_ts\"])\n",
    "\n",
    "# -----------------------------------------\n",
    "# 7) Gold sanity checks (ops dashboard not empty)\n",
    "# -----------------------------------------\n",
    "ops_cnt = gold_ops.count()\n",
    "metric(f\"{DB_GOLD}.ops_daily_dashboard\", \"GOLD\", \"rowcount\", ops_cnt)\n",
    "\n",
    "if ops_cnt == 0:\n",
    "    write_dq(\n",
    "        dataset=\"ops_daily_dashboard\",\n",
    "        layer=\"GOLD\",\n",
    "        rule_name=\"GOLD_EMPTY_TABLE\",\n",
    "        severity=\"CRITICAL\",\n",
    "        df_violations=gold_ops,  # empty\n",
    "        key_cols=[\"event_date\",\"region\"]\n",
    "    )\n",
    "\n",
    "# -----------------------------------------\n",
    "# 8) Print judge-friendly report\n",
    "# -----------------------------------------\n",
    "print(\"\\n======================\")\n",
    "print(\"✅ DATA QUALITY REPORT\")\n",
    "print(\"======================\\n\")\n",
    "\n",
    "print(\"Latest run batch_id:\", BATCH_ID)\n",
    "\n",
    "print(\"\\n-- DQ Violations (this run) --\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT severity, dataset, rule_name, violation_count, sample_keys\n",
    "FROM {DB_AUDIT}.dq_violations\n",
    "WHERE batch_id = '{BATCH_ID}'\n",
    "ORDER BY\n",
    "  CASE severity WHEN 'CRITICAL' THEN 1 WHEN 'WARNING' THEN 2 ELSE 3 END,\n",
    "  dataset, rule_name\n",
    "\"\"\").show(200, truncate=False)\n",
    "\n",
    "print(\"\\n-- Pipeline Metrics (this run) --\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT layer, table_fqn, metric_name, metric_value\n",
    "FROM {DB_AUDIT}.table_metrics\n",
    "WHERE batch_id = '{BATCH_ID}'\n",
    "ORDER BY layer, table_fqn, metric_name\n",
    "\"\"\").show(200, truncate=False)\n",
    "\n",
    "print(\"\\n-- Gold Executive Snapshot (sample) --\")\n",
    "spark.table(f\"{DB_GOLD}.ops_daily_dashboard\").orderBy(F.col(\"event_date\").desc()).show(30, truncate=False)\n",
    "\n",
    "print(\"\\n✅ DQ + audit report completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_dq_audit_report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
