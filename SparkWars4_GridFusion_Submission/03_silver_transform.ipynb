{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21bcfb0f-db51-41b8-a120-6bb14caf7344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks notebook source\n",
    "# =========================================\n",
    "# Notebook: 03_silver_transform\n",
    "# Purpose : Curate Silver layer:\n",
    "#          - type casting + standardization\n",
    "#          - dedup\n",
    "#          - billing CDC apply via MERGE\n",
    "#          - transformer SCD2 via MERGE\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb3b969-b066-4243-8fe2-05de16451f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125befae-88eb-4abc-a2d9-8b9ca91e9025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2331d0-0671-4b90-aee4-eaefe28ca84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_as_table(df, db, table, path, mode=\"overwrite\"):\n",
    "    (df.write.format(\"delta\").mode(mode).save(path))\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {db}.{table} USING DELTA LOCATION '{path}'\")\n",
    "\n",
    "def table_exists(db, table):\n",
    "    try:\n",
    "        spark.table(f\"{db}.{table}\")\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def delta_exists(path):\n",
    "    try:\n",
    "        return DeltaTable.isDeltaTable(spark, path)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f989ca42-6a33-4237-94ca-f9f3bf81cf97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 1) Smart Meter Silver: clean + dedup + anomaly scoring\n",
    "# -----------------------------------------\n",
    "sm_bronze = spark.table(f\"{DB_BRONZE}.smart_meter_events\")\n",
    "\n",
    "sm = (sm_bronze\n",
    "    .withColumn(\"event_ts\", F.to_timestamp(\"timestamp\"))\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    "    .drop(\"timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d89253-7e34-4c55-a102-5736fa17c98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dedup: keep latest record for each event_id (or fallback to meter_id+event_ts)\n",
    "# We use ingestion timestamp as tie-breaker (deterministic).\n",
    "w_sm = W.partitionBy(\"event_id\").orderBy(F.col(\"_ingestion_ts\").desc(), F.col(\"_source_file\").desc())\n",
    "sm_dedup = (sm\n",
    "    .withColumn(\"_rn\", F.row_number().over(w_sm))\n",
    "    .filter(F.col(\"_rn\") == 1)\n",
    "    .drop(\"_rn\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bf95282-4eb3-4de9-8b7f-217e178cc319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Robust anomaly thresholds per region (1st & 99th percentile using approx)\n",
    "# (Edge: dynamic thresholds instead of hard-coded)\n",
    "thr = (sm_dedup\n",
    "    .groupBy(\"region\")\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(kwh_consumed, 0.01)\").alias(\"p01_kwh\"),\n",
    "        F.expr(\"percentile_approx(kwh_consumed, 0.99)\").alias(\"p99_kwh\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "sm_scored = (sm_dedup\n",
    "    .join(thr, on=\"region\", how=\"left\")\n",
    "    .withColumn(\"is_kwh_anomaly\",\n",
    "        (F.col(\"kwh_consumed\") < F.col(\"p01_kwh\")) | (F.col(\"kwh_consumed\") > F.col(\"p99_kwh\"))\n",
    "    )\n",
    "    .withColumn(\"anomaly_type\",\n",
    "        F.when(F.col(\"kwh_consumed\") < F.col(\"p01_kwh\"), F.lit(\"LOW_CONSUMPTION\"))\n",
    "         .when(F.col(\"kwh_consumed\") > F.col(\"p99_kwh\"), F.lit(\"HIGH_CONSUMPTION\"))\n",
    "         .otherwise(F.lit(None))\n",
    "    )\n",
    "    .drop(\"p01_kwh\",\"p99_kwh\")\n",
    ")\n",
    "\n",
    "sm_silver_path = f\"{PATH_SILVER}/smart_meter_readings\"\n",
    "save_as_table(sm_scored, DB_SILVER, \"smart_meter_readings\", sm_silver_path, mode=\"overwrite\")\n",
    "\n",
    "audit_table_metrics(f\"{DB_SILVER}.smart_meter_readings\", \"SILVER\", \"rowcount\", str(sm_scored.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb1043bf-5de8-420a-b576-9c865daa4144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 2) Substation Telemetry Silver: clean + dedup + transformer risk score\n",
    "# -----------------------------------------\n",
    "tel_bronze = spark.table(f\"{DB_BRONZE}.substation_telemetry_events\")\n",
    "\n",
    "tel = (tel_bronze\n",
    "    .withColumn(\"telemetry_ts\", F.to_timestamp(\"timestamp\"))\n",
    "    .withColumn(\"event_date\", F.to_date(\"telemetry_ts\"))\n",
    "    .drop(\"timestamp\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8197850f-9a8a-403e-a167-d25b74cae0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dedup by telemetry_id\n",
    "w_tel = W.partitionBy(\"telemetry_id\").orderBy(F.col(\"_ingestion_ts\").desc(), F.col(\"_source_file\").desc())\n",
    "tel_dedup = (tel\n",
    "    .withColumn(\"_rn\", F.row_number().over(w_tel))\n",
    "    .filter(F.col(\"_rn\") == 1)\n",
    "    .drop(\"_rn\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be048b70-633e-4a3b-8be7-aae2b3c57a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Risk scoring (simple + explainable, but strong story)\n",
    "tel_scored = (tel_dedup\n",
    "    .withColumn(\"oil_temp_delta_c\", F.col(\"oil_temperature_c\") - F.col(\"ambient_temperature_c\"))\n",
    "    .withColumn(\"risk_alarm\", F.when(F.col(\"alarm_code\").isNotNull() & (F.col(\"alarm_code\") != \"\"), 1).otherwise(0))\n",
    "    .withColumn(\"risk_oil_temp\", F.when(F.col(\"oil_temp_delta_c\") > 25, 1).otherwise(0))\n",
    "    .withColumn(\"risk_gas\", F.when(F.col(\"dissolved_gas_ppm\") > 800, 1).otherwise(0))\n",
    "    .withColumn(\"risk_vibration\", F.when(F.col(\"vibration_mm_s\") > 7, 1).otherwise(0))\n",
    "    .withColumn(\"risk_overload\", F.when(F.col(\"load_pct\") > 90, 1).otherwise(0))\n",
    "    .withColumn(\"risk_score\",\n",
    "        3*F.col(\"risk_alarm\") +\n",
    "        2*F.col(\"risk_oil_temp\") +\n",
    "        2*F.col(\"risk_gas\") +\n",
    "        1*F.col(\"risk_vibration\") +\n",
    "        1*F.col(\"risk_overload\")\n",
    "    )\n",
    "    .withColumn(\"risk_level\",\n",
    "        F.when(F.col(\"risk_score\") >= 6, F.lit(\"HIGH\"))\n",
    "         .when(F.col(\"risk_score\") >= 3, F.lit(\"MEDIUM\"))\n",
    "         .otherwise(F.lit(\"LOW\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "tel_silver_path = f\"{PATH_SILVER}/substation_telemetry\"\n",
    "save_as_table(tel_scored, DB_SILVER, \"substation_telemetry\", tel_silver_path, mode=\"overwrite\")\n",
    "audit_table_metrics(f\"{DB_SILVER}.substation_telemetry\", \"SILVER\", \"rowcount\", str(tel_scored.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc62c953-e1c2-4c46-86af-af58a47faadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 3) Maintenance Logs Silver: standardize\n",
    "# -----------------------------------------\n",
    "mnt_bronze = spark.table(f\"{DB_BRONZE}.maintenance_logs\")\n",
    "\n",
    "# Standardize likely columns (we keep flexible if schema changes)\n",
    "# If your CSV has different names, we’ll keep columns and only standardize dates if present.\n",
    "mnt = mnt_bronze\n",
    "for c in [\"maintenance_date\", \"performed_at\", \"date\"]:\n",
    "    if c in mnt.columns:\n",
    "        mnt = mnt.withColumn(\"maintenance_ts\", F.to_timestamp(F.col(c)))\n",
    "        break\n",
    "\n",
    "mnt_silver_path = f\"{PATH_SILVER}/maintenance_logs\"\n",
    "save_as_table(mnt, DB_SILVER, \"maintenance_logs\", mnt_silver_path, mode=\"overwrite\")\n",
    "audit_table_metrics(f\"{DB_SILVER}.maintenance_logs\", \"SILVER\", \"rowcount\", str(mnt.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b7cba70-c77c-4348-bcf7-81a61c62fd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 4) Customer Dimension Silver: SCD2-ready snapshot\n",
    "# -----------------------------------------\n",
    "cust_bronze = spark.table(f\"{DB_BRONZE}.customer_master_snapshot\")\n",
    "\n",
    "# If dataset already contains SCD2 fields, keep them. Else add baseline.\n",
    "cust = cust_bronze\n",
    "if \"is_current\" not in cust.columns:\n",
    "    cust = (cust\n",
    "        .withColumn(\"record_effective_date\", F.to_date(F.lit(\"1900-01-01\")))\n",
    "        .withColumn(\"record_end_date\", F.to_date(F.lit(\"9999-12-31\")))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "        .withColumn(\"scd_version\", F.lit(1))\n",
    "    )\n",
    "\n",
    "cust_silver_path = f\"{PATH_SILVER}/dim_customer\"\n",
    "save_as_table(cust, DB_SILVER, \"dim_customer\", cust_silver_path, mode=\"overwrite\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW {DB_SILVER}.dim_customer_current AS SELECT * FROM {DB_SILVER}.dim_customer WHERE is_current = true\")\n",
    "audit_table_metrics(f\"{DB_SILVER}.dim_customer\", \"SILVER\", \"rowcount\", str(cust.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e471fac-6a3c-4807-a09e-06be7754d74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 4) Customer Dimension Silver: SCD2-ready snapshot\n",
    "# -----------------------------------------\n",
    "cust_bronze = spark.table(f\"{DB_BRONZE}.customer_master_snapshot\")\n",
    "\n",
    "# If dataset already contains SCD2 fields, keep them. Else add baseline.\n",
    "cust = cust_bronze\n",
    "if \"is_current\" not in cust.columns:\n",
    "    cust = (cust\n",
    "        .withColumn(\"record_effective_date\", F.to_date(F.lit(\"1900-01-01\")))\n",
    "        .withColumn(\"record_end_date\", F.to_date(F.lit(\"9999-12-31\")))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "        .withColumn(\"scd_version\", F.lit(1))\n",
    "    )\n",
    "\n",
    "cust_silver_path = f\"{PATH_SILVER}/dim_customer\"\n",
    "save_as_table(cust, DB_SILVER, \"dim_customer\", cust_silver_path, mode=\"overwrite\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW {DB_SILVER}.dim_customer_current AS SELECT * FROM {DB_SILVER}.dim_customer WHERE is_current = true\")\n",
    "audit_table_metrics(f\"{DB_SILVER}.dim_customer\", \"SILVER\", \"rowcount\", str(cust.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9daae1a-1745-464f-929d-ff697e35e70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize dim if not exists\n",
    "if not delta_exists(trf_silver_path):\n",
    "    trf_snap.write.format(\"delta\").mode(\"overwrite\").save(trf_silver_path)\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_SILVER}.dim_transformer USING DELTA LOCATION '{trf_silver_path}'\")\n",
    "else:\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_SILVER}.dim_transformer USING DELTA LOCATION '{trf_silver_path}'\")\n",
    "\n",
    "# Determine business key\n",
    "# (Assume transformer_id exists; if not, fail fast)\n",
    "assert \"transformer_id\" in trf_delta.columns, \"transformer_id not found in transformer delta\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c001af2b-fbb0-44de-bbdf-8e7c36e021a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Columns to compare for change detection (exclude SCD + metadata)\n",
    "scd_cols = {\"record_effective_date\",\"record_end_date\",\"is_current\",\"scd_version\"}\n",
    "meta_cols = {c for c in trf_delta.columns if c.startswith(\"_\")}\n",
    "compare_cols = [c for c in trf_delta.columns if c not in scd_cols and c not in meta_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abc778b0-3857-45f1-a04f-e505a15f115e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join to compute next version if needed\n",
    "dim_current = spark.read.format(\"delta\").load(trf_silver_path).filter(\"is_current = true\").select(\"transformer_id\",\"scd_version\")\n",
    "incoming = (trf_delta.alias(\"d\")\n",
    "    .join(dim_current.alias(\"c\"), on=\"transformer_id\", how=\"left\")\n",
    "    .withColumn(\"next_scd_version\", F.coalesce(F.col(\"c.scd_version\") + F.lit(1), F.lit(1)))\n",
    "    .drop(\"scd_version\")  # we will use next_scd_version instead\n",
    "    .withColumnRenamed(\"next_scd_version\", \"scd_version\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13b03348-b417-427b-b942-2122c1c498a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join to compute next version if needed\n",
    "dim_current = spark.read.format(\"delta\").load(trf_silver_path).filter(\"is_current = true\").select(\"transformer_id\",\"scd_version\")\n",
    "incoming = (trf_delta.alias(\"d\")\n",
    "    .join(dim_current.alias(\"c\"), on=\"transformer_id\", how=\"left\")\n",
    "    .withColumn(\"next_scd_version\", F.coalesce(F.col(\"c.scd_version\") + F.lit(1), F.lit(1)))\n",
    "    .drop(\"scd_version\")  # we will use next_scd_version instead\n",
    "    .withColumnRenamed(\"next_scd_version\", \"scd_version\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af66582c-1a21-4aa5-8197-de9bd9b9daef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE: expire current row if changed; insert new row as current\n",
    "dt = DeltaTable.forPath(spark, trf_silver_path)\n",
    "\n",
    "(dt.alias(\"t\")\n",
    " .merge(incoming.alias(\"s\"), \"t.transformer_id = s.transformer_id AND t.is_current = true\")\n",
    " .whenMatchedUpdate(\n",
    "     condition=change_pred,\n",
    "     set={\n",
    "         \"record_end_date\": F.date_sub(F.to_date(F.col(\"s.record_effective_date\")), 1),\n",
    "         \"is_current\": F.lit(False)\n",
    "     }\n",
    " )\n",
    " .whenNotMatchedInsert(values={c: F.col(f\"s.{c}\") for c in incoming.columns})\n",
    " .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0062f33-2651-435b-80fa-4439403db041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# After expiring, we must insert new versions for changed keys.\n",
    "# Keys that were matched+changed won't insert in above merge (because they matched).\n",
    "# So we insert them explicitly.\n",
    "changed_keys = (spark.read.format(\"delta\").load(trf_silver_path)\n",
    "    .filter(\"is_current = false\")\n",
    "    .select(\"transformer_id\")\n",
    "    .join(incoming.select(\"transformer_id\").distinct(), on=\"transformer_id\", how=\"inner\")\n",
    "    .select(\"transformer_id\").distinct()\n",
    ")\n",
    "\n",
    "to_insert = incoming.join(changed_keys, on=\"transformer_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdaa02c1-d212-4ec8-b6b2-4efece3cb306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert new current versions\n",
    "(to_insert\n",
    " .write.format(\"delta\")\n",
    " .mode(\"append\")\n",
    " .save(trf_silver_path)\n",
    ")\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW {DB_SILVER}.dim_transformer_current AS SELECT * FROM {DB_SILVER}.dim_transformer WHERE is_current = true\")\n",
    "audit_table_metrics(f\"{DB_SILVER}.dim_transformer\", \"SILVER\", \"rowcount\", str(spark.table(f\"{DB_SILVER}.dim_transformer\").count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf7004e-c4b4-44c2-8e34-c090c74882f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 6) Renewable Production Silver: standardize schema evolution output\n",
    "# -----------------------------------------\n",
    "renew_bronze = spark.table(f\"{DB_BRONZE}.renewable_production\")\n",
    "\n",
    "# Make timestamp column consistent\n",
    "# Many JSON feeds include 'timestamp' or can derive from filename. We'll handle both.\n",
    "renew = renew_bronze\n",
    "if \"timestamp\" in renew.columns:\n",
    "    renew = renew.withColumn(\"event_ts\", F.to_timestamp(\"timestamp\")).drop(\"timestamp\")\n",
    "elif \"datetime\" in renew.columns:\n",
    "    renew = renew.withColumn(\"event_ts\", F.to_timestamp(\"datetime\")).drop(\"datetime\")\n",
    "else:\n",
    "    # Fallback: attempt parse from filename like renewable_production_YYYYMMDD_HHMM.json\n",
    "    renew = (renew\n",
    "        .withColumn(\"_fn\", F.regexp_extract(F.col(\"_source_file\"), r\"renewable_production_(\\d{8})_(\\d{4})\", 0))\n",
    "        .withColumn(\"event_ts\",\n",
    "            F.to_timestamp(\n",
    "                F.concat_ws(\" \",\n",
    "                    F.regexp_extract(F.col(\"_source_file\"), r\"renewable_production_(\\d{8})_\", 1),\n",
    "                    F.regexp_extract(F.col(\"_source_file\"), r\"_(\\d{4})\\.json\", 1)\n",
    "                ),\n",
    "                \"yyyyMMdd HHmm\"\n",
    "            )\n",
    "        )\n",
    "        .drop(\"_fn\")\n",
    "    )\n",
    "\n",
    "renew = renew.withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    "\n",
    "# Ensure evolving columns exist (curtailment_mw, battery_storage_mwh) even if null\n",
    "for c in [\"curtailment_mw\", \"battery_storage_mwh\"]:\n",
    "    if c not in renew.columns:\n",
    "        renew = renew.withColumn(c, F.lit(None).cast(\"double\"))\n",
    "\n",
    "renew_silver_path = f\"{PATH_SILVER}/renewable_production\"\n",
    "save_as_table(renew, DB_SILVER, \"renewable_production\", renew_silver_path, mode=\"overwrite\")\n",
    "audit_table_metrics(f\"{DB_SILVER}.renewable_production\", \"SILVER\", \"rowcount\", str(renew.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19eae633-bf44-4044-aea4-b8e68763b9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 7) Billing CDC Apply -> Silver current-state table (MERGE)\n",
    "# -----------------------------------------\n",
    "billing_bronze = spark.table(f\"{DB_BRONZE}.billing_cdc_events\")\n",
    "\n",
    "# Standardize types\n",
    "bill = (billing_bronze\n",
    "    .withColumn(\"cdc_ts\", F.to_timestamp(\"cdc_timestamp\"))\n",
    "    .withColumn(\"billing_period_start\", F.to_date(\"billing_period_start\"))\n",
    "    .withColumn(\"billing_period_end\", F.to_date(\"billing_period_end\"))\n",
    "    .withColumn(\"due_date\", F.to_date(\"due_date\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f65d41-e6d0-40e7-8aa7-afa9d5b36ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CDC semantics:\n",
    "# 1 = DELETE, 2 = INSERT, 3 = BEFORE-IMAGE (ignore), 4 = AFTER-IMAGE/UPDATE (upsert)\n",
    "# We'll use transaction_id as natural key (exists in the file).\n",
    "assert \"transaction_id\" in bill.columns, \"Expected transaction_id in billing CDC feed\"\n",
    "\n",
    "# Keep latest event per transaction_id (tie-break seqval if present)\n",
    "order_cols = [F.col(\"cdc_ts\").desc()]\n",
    "if \"__$seqval\" in bill.columns:\n",
    "    order_cols.append(F.col(\"__$seqval\").desc())\n",
    "order_cols.append(F.col(\"_ingestion_ts\").desc())\n",
    "\n",
    "w_cdc = W.partitionBy(\"transaction_id\").orderBy(*order_cols)\n",
    "bill_latest = (bill\n",
    "    .withColumn(\"_rn\", F.row_number().over(w_cdc))\n",
    "    .filter(F.col(\"_rn\") == 1)\n",
    "    .drop(\"_rn\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c2394c-195a-46ec-b6bc-da61025ace68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split actions\n",
    "bill_upserts = bill_latest.filter(F.col(\"__$operation\").isin([2,4]))\n",
    "bill_deletes = bill_latest.filter(F.col(\"__$operation\") == 1).select(\"transaction_id\").distinct()\n",
    "\n",
    "billing_silver_path = f\"{PATH_SILVER}/billing_transactions_current\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "309d721e-a206-40bd-92f2-3e1580673f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize target table if needed\n",
    "if not delta_exists(billing_silver_path):\n",
    "    # Create empty Delta with the upsert schema\n",
    "    (bill_upserts.limit(0)\n",
    "        .write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(billing_silver_path)\n",
    "    )\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_SILVER}.billing_transactions_current USING DELTA LOCATION '{billing_silver_path}'\")\n",
    "else:\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_SILVER}.billing_transactions_current USING DELTA LOCATION '{billing_silver_path}'\")\n",
    "\n",
    "target = DeltaTable.forPath(spark, billing_silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b9b61e0-a07f-4ddf-a18e-7c2f67f9855e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply deletes first (clean)\n",
    "if bill_deletes.count() > 0:\n",
    "    (target.alias(\"t\")\n",
    "        .merge(bill_deletes.alias(\"s\"), \"t.transaction_id = s.transaction_id\")\n",
    "        .whenMatchedDelete()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "# Apply upserts\n",
    "# (insert new rows, update existing)\n",
    "if bill_upserts.count() > 0:\n",
    "    set_map = {c: F.col(f\"s.{c}\") for c in bill_upserts.columns}\n",
    "    (target.alias(\"t\")\n",
    "        .merge(bill_upserts.alias(\"s\"), \"t.transaction_id = s.transaction_id\")\n",
    "        .whenMatchedUpdate(set=set_map)\n",
    "        .whenNotMatchedInsert(values=set_map)\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "audit_table_metrics(f\"{DB_SILVER}.billing_transactions_current\", \"SILVER\", \"rowcount\", str(spark.table(f\"{DB_SILVER}.billing_transactions_current\").count()))\n",
    "\n",
    "print(\"✅ Silver transforms completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
