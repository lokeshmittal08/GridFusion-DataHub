{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d59e5c4-23aa-4079-8054-4e01689d9edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45481a6e-ca10-41da-a443-1dda0b1574b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57bbbbf-ff09-461c-b101-f7fff2893554",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "def rm_dbfs(path):\n",
    "    try:\n",
    "        dbutils.fs.rm(path, recurse=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if RESET_CHECKPOINTS:\n",
    "    rm_dbfs(PATH_CHECKPOINTS)\n",
    "    rm_dbfs(PATH_SCHEMAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f90e346-faac-426e-8f1c-048748b083e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_meter_bronze_path = f\"{PATH_BRONZE}/smart_meter_events\"\n",
    "smart_meter_ckpt = f\"{PATH_CHECKPOINTS}/smart_meter\"\n",
    "df_sm_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .schema(SMART_METER_SCHEMA)\n",
    "    .load(f\"{SRC_SMART_METER}/smart_meter_stream.json\")\n",
    ")\n",
    "df_sm_bronze = add_ingestion_cols(df_sm_stream, \"smart_meter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7213c7a8-a9e3-4530-9402-2eae29ec14e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(sm_write := df_sm_bronze.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", smart_meter_ckpt)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(smart_meter_bronze_path)\n",
    ")\n",
    "sm_write.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee6397e-b5fb-40d1-826a-30fb8ec40ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.smart_meter_events USING DELTA LOCATION '{smart_meter_bronze_path}'\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.smart_meter_events\", \"BRONZE\", \"status\", \"loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a66ce6-3bcf-48c4-b2d2-ab522a7950e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Substation Telemetry — file stream\n",
    "# ----------------------------\n",
    "telemetry_bronze_path = f\"{PATH_BRONZE}/substation_telemetry_events\"\n",
    "telemetry_ckpt = f\"{PATH_CHECKPOINTS}/substation_telemetry\"\n",
    "\n",
    "df_tel_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .schema(SUBSTATION_SCHEMA)\n",
    "    .load(f\"{SRC_SUBSTATION}/substation_telemetry_stream.json\")\n",
    ")\n",
    "df_tel_bronze = add_ingestion_cols(df_tel_stream, \"substation_telemetry\")\n",
    "\n",
    "(tel_write := df_tel_bronze.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", telemetry_ckpt)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(telemetry_bronze_path)\n",
    ")\n",
    "tel_write.awaitTermination()\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.substation_telemetry_events USING DELTA LOCATION '{telemetry_bronze_path}'\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.substation_telemetry_events\", \"BRONZE\", \"status\", \"loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759e39de-128e-436d-9878-2423d7227f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Billing CDC — batch to bronze (raw events)\n",
    "# CDC feed is line-delimited JSON with __$operation :contentReference[oaicite:6]{index=6}\n",
    "# ----------------------------\n",
    "billing_bronze_path = f\"{PATH_BRONZE}/billing_cdc_events\"\n",
    "df_billing_raw = spark.read.json(f\"{SRC_BILLING_CDC}/billing_cdc_feed.json\")\n",
    "df_billing_bronze = add_ingestion_cols(df_billing_raw, \"billing_cdc\")\n",
    "\n",
    "(df_billing_bronze.write.format(\"delta\")\n",
    "  .mode(\"append\" if RUN_MODE == \"INCR\" else \"overwrite\")\n",
    "  .save(billing_bronze_path)\n",
    ")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.billing_cdc_events USING DELTA LOCATION '{billing_bronze_path}'\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.billing_cdc_events\", \"BRONZE\", \"status\", \"loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97913282-e13a-4c0e-9383-7f2d515dfe8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) Maintenance CSV — batch (3 monthly files) :contentReference[oaicite:7]{index=7}\n",
    "# ----------------------------\n",
    "maintenance_bronze_path = f\"{PATH_BRONZE}/maintenance_logs\"\n",
    "df_maint_raw = (spark.read\n",
    "    .format(\"csv\").option(\"header\", True).option(\"inferSchema\", True)\n",
    "    .load(f\"{SRC_MAINTENANCE}/*.csv\")\n",
    ")\n",
    "df_maint_bronze = add_ingestion_cols(df_maint_raw, \"maintenance_csv\")\n",
    "\n",
    "(df_maint_bronze.write.format(\"delta\")\n",
    "  .mode(\"append\" if RUN_MODE == \"INCR\" else \"overwrite\")\n",
    "  .save(maintenance_bronze_path)\n",
    ")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.maintenance_logs USING DELTA LOCATION '{maintenance_bronze_path}'\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.maintenance_logs\", \"BRONZE\", \"status\", \"loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18af95b-eb76-4cb5-b52e-de8e2a2ef490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5) Transformer + Customer Parquet (batch snapshots + delta)\n",
    "# Transformer is SCD2 snapshot + delta :contentReference[oaicite:8]{index=8}\n",
    "# Customer is SCD2 snapshot :contentReference[oaicite:9]{index=9}\n",
    "# ----------------------------\n",
    "trf_snap_path = f\"{PATH_BRONZE}/transformer_master_snapshot\"\n",
    "trf_delta_path = f\"{PATH_BRONZE}/transformer_master_delta\"\n",
    "cust_snap_path = f\"{PATH_BRONZE}/customer_master_snapshot\"\n",
    "\n",
    "df_trf = spark.read.parquet(f\"{SRC_TRANSFORMERS}/transformer_master.parquet\")\n",
    "df_trf_delta = spark.read.parquet(f\"{SRC_TRANSFORMERS}/transformer_master_delta_20250601.parquet\")\n",
    "df_cust = spark.read.parquet(f\"{SRC_TRANSFORMERS}/customer_master.parquet\")\n",
    "\n",
    "add_ingestion_cols(df_trf, \"transformer_master\").write.format(\"delta\").mode(\"overwrite\").save(trf_snap_path)\n",
    "add_ingestion_cols(df_trf_delta, \"transformer_master_delta\").write.format(\"delta\").mode(\"overwrite\").save(trf_delta_path)\n",
    "add_ingestion_cols(df_cust, \"customer_master\").write.format(\"delta\").mode(\"overwrite\").save(cust_snap_path)\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.transformer_master_snapshot USING DELTA LOCATION '{trf_snap_path}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.transformer_master_delta USING DELTA LOCATION '{trf_delta_path}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.customer_master_snapshot USING DELTA LOCATION '{cust_snap_path}'\")\n",
    "\n",
    "audit_table_metrics(f\"{DB_BRONZE}.transformer_master_snapshot\", \"BRONZE\", \"status\", \"loaded\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.transformer_master_delta\", \"BRONZE\", \"status\", \"loaded\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.customer_master_snapshot\", \"BRONZE\", \"status\", \"loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93f621c-33ce-4029-9624-e72ef3b099e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6) Renewable Production — Auto Loader schema evolution :contentReference[oaicite:10]{index=10}\n",
    "# ----------------------------\n",
    "renew_bronze_path = f\"{PATH_BRONZE}/renewable_production\"\n",
    "renew_ckpt = f\"{PATH_CHECKPOINTS}/renewable\"\n",
    "renew_schema_loc = f\"{PATH_SCHEMAS}/renewable\"\n",
    "\n",
    "df_renew = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", True)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "    .option(\"cloudFiles.schemaLocation\", renew_schema_loc)\n",
    "    .option(\"cloudFiles.schemaHints\", \"production_mw double, curtailment_mw double, battery_storage_mwh double\")\n",
    "    .load(f\"{SRC_RENEWABLE}/*.json\")\n",
    ")\n",
    "df_renew_bronze = add_ingestion_cols(df_renew, \"renewable_production\")\n",
    "\n",
    "(renew_write := df_renew_bronze.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", renew_ckpt)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(renew_bronze_path)\n",
    ")\n",
    "renew_write.awaitTermination()\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {DB_BRONZE}.renewable_production USING DELTA LOCATION '{renew_bronze_path}'\")\n",
    "audit_table_metrics(f\"{DB_BRONZE}.renewable_production\", \"BRONZE\", \"status\", \"loaded\")\n",
    "\n",
    "# Basic rowcount metrics\n",
    "for t in [\"smart_meter_events\",\"substation_telemetry_events\",\"billing_cdc_events\",\"maintenance_logs\",\"transformer_master_snapshot\",\"transformer_master_delta\",\"customer_master_snapshot\",\"renewable_production\"]:\n",
    "    cnt = spark.table(f\"{DB_BRONZE}.{t}\").count()\n",
    "    audit_table_metrics(f\"{DB_BRONZE}.{t}\", \"BRONZE\", \"rowcount\", str(cnt))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
